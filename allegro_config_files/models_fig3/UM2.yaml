# The config file is divided into 4 sections -- `data`, `train`, `model`, and `global_options`
# The config system relies on omegaconf (https://omegaconf.readthedocs.io/en/2.3_branch/index.html)
# and hydra (https://hydra.cc/docs/intro/) functionalities, such as
# - omegaconf's variable interpolation (https://omegaconf.readthedocs.io/en/2.3_branch/usage.html#variable-interpolation)
# - omegaconf's resolvers (https://omegaconf.readthedocs.io/en/2.3_branch/usage.html#resolvers)
# - hydra's instantiate (https://hydra.cc/docs/advanced/instantiate_objects/overview/)
# With hydra's instantiation (notice the "_target_"s everywhere), the config file (almost) directly corresponds
# to instantiating objects as one would normally do in Python.
# Much of the infrastructure is based on PyTorch Lightning (https://lightning.ai/docs/pytorch/stable/),
# such as the use of Lightning's Trainer, DataModule, LightningModule, Callback objects.


# the run types will be completed in sequence
# one can do `train`, `val`, `test`, `predict` run types
run: [train]


# the following parameters (cutoff_radius, chemical_symbols, model_type_names) are not used direcly by the code
# parameters that take thier values show up multiple times in the config, so this allows us to use
# variable interpolation to keep their multiple instances consistent


# There are two sets of atomic types to keep track of in most applications
# -- there is the conventional atomic species (e.g. C, H), and a separate `type_names` known to the model.
# The model only knows types based on a set of zero-based indices and user-given `type_names` argument.
# An example where this distinction is necessary include datasets with the same atomic species with different charge states:
# we could define `chemical_symbols: [C, C]` and model `type_names: [C3, C4]` for +3 and +4 charge states.
# There could also be instances such as coarse graining we only care about the model's `type_names` (no need to define chemical species).
# Because of this distinction, these variables show up as arguments across different categories, including, data, model, metrics and even callbacks.
# In this case, we fix both to be the same, so we define a single set of each here and use variable interpolation to retrieve them below.
# This ensures a single location where the values are set to reduce the chances of mis-configuring runs.
chemical_symbols: [Mo, S, W, Se] 
model_type_names: ${chemical_symbols}


# data is managed by LightningDataModules
# nequip provides some standard datamodules that can be found in nequip.data.datamodule
# users are free to define and use their own datamodules that subclass nequip.data.datamodule.NequIPDataModule
data:
  _target_: nequip.data.datamodule.ASEDataModule
  seed: 456             # dataset seed for reproducibility
  
  # here we take an ASE-readable file (in extxyz format) and split it into train:val:test = 80:10:10
  split_dataset:
    file_path: ../datasets/MoS2_WSe2_Unified_train.xyz
    train: 0.8
    val: 0.2


  # `transforms` convert data from the Dataset to a form that can be used by the ML model
  # the transforms are only performed right before data is given to the model
  # data is kept in its untransformed form
  
  transforms:
    # data doesn't usually come with a neighborlist -- this tranforms prepares the neighborlist
    - _target_: nequip.data.transforms.NeighborListTransform
      r_max: 6.0
    # the models only know atom types, which can be different from the chemical species (e.g. C, H)
    # for instance we can have data with different charge states of carbon, which means they are
    # all labeled by chemical species `C`, but may have different atom type labels based on the charge states
    # in this case, the atom types are the same as the chemical species, but we still have to include this
    # transformation to ensure that the data has 0-indexed atom type lists used in the various model operations 
    - _target_: nequip.data.transforms.ChemicalSpeciesToAtomTypeMapper
      chemical_symbols: ${chemical_symbols}

  # the following are torch.utils.data.dataloader arguments except for `dataset` and `collate_fn`
  # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader
  train_dataloader_kwargs:
    batch_size: 5
    num_workers: 5
    shuffle: true
  val_dataloader_kwargs:
    batch_size: 10
    num_workers: ${data.train_dataloader_kwargs.num_workers}  # we want to use the same num_workers -- variable interpolation helps
  test_dataloader_kwargs: ${data.val_dataloader_kwargs}  # variable interpolation comes in handy again

  # dataset statistics can be calculated to be used for model initialization such as for shifting, scaling and standardizing.
  # it is advised to provide custom names -- you will have to retrieve them later under model to initialize certain parameters to the dataset statistics computed
  stats_manager:
    # dataset statistics is handled by the DataStatisticsManager
    _target_: nequip.data.DataStatisticsManager
    # dataloader kwargs for data statistics computation
    # `batch_size` should ideally be as large as possible without trigerring OOM
    dataloader_kwargs:
      batch_size: 10
    metrics:
      - field:
          _target_: nequip.data.NumNeighbors
        metric: 
          _target_: nequip.data.Mean
        name: num_neighbors_mean
      - field:
          _target_: nequip.data.PerAtomModifier
          field: total_energy
        metric:
          _target_: nequip.data.Mean
        name: per_atom_energy_mean
      # we can also compute per_type statistics
      - field: forces
        metric:
          _target_: nequip.data.RootMeanSquare
        per_type: true
        name: per_type_forces_rms
      # or compute the regular ones
      #- field: forces
      #  metric:
      #    _target_: nequip.data.RootMeanSquare
      #  name: forces_rms


# `trainer` (mandatory) is a Lightning.Trainer object (https://lightning.ai/docs/pytorch/stable/common/trainer.html#trainer-class-api)
trainer:
  _target_: lightning.Trainer
  accelerator: gpu
  enable_checkpointing: true
  max_epochs: 1000
  max_time: 03:00:00:00
  check_val_every_n_epoch: 1  # how often to validate
  log_every_n_steps: 20       # how often to log

  # use any Lightning supported logger
  logger:
    # Lightning wandb logger https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.wandb.html#module-lightning.pytorch.loggers.wandb
    _target_: lightning.pytorch.loggers.wandb.WandbLogger
    project: newalg_MoS2WSe2
    name: baseline_v2_r16_2
    save_dir: ${hydra:runtime.output_dir}  # use resolver to place wandb logs in hydra's output directory

  # use any Lightning callbacks https://lightning.ai/docs/pytorch/stable/api_references.html#callbacks
  # and any custom callbakcs that subclass Lightning's Callback parent class
  callbacks:
    # Common callbacks used in ML

    # stop training when some criterion is met
    - _target_: lightning.pytorch.callbacks.EarlyStopping
      monitor: val0_epoch/weighted_sum        # validation metric to monitor
      min_delta: 1e-5                         # how much to be considered a "change"
      patience: 100                            # how many instances of "no change" before stopping

    # checkpoint based on some criterion
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      monitor: val0_epoch/weighted_sum        # validation metric to monitor
      dirpath: ${hydra:runtime.output_dir}    # use hydra output directory
      filename: best                          # best.ckpt is the checkpoint name
      save_last: true                         # last.ckpt will be saved
      
    # log learning rate, e.g. to monitor what the learning rate scheduler is doing
    - _target_: lightning.pytorch.callbacks.LearningRateMonitor
      logging_interval: epoch

    # use EMA for smoother validation curves and thus more reliable metrics for monitoring
    #- _target_: nequip.train.callbacks.NeMoExponentialMovingAverage
    #  decay: 0.99
    #  every_n_steps: 1

    # or use Lightning's SWA
    - _target_: lightning.pytorch.callbacks.StochasticWeightAveraging
      swa_lrs: 1e-4
      swa_epoch_start: 50
      annealing_epochs: 20

    # Callbacks to handle loss coefficients to balance different objectives (energy, forces, etc)

    # SoftAdapt scheme (https://arxiv.org/abs/2403.18122) to adaptively change loss coefficients
    - _target_: nequip.train.callbacks.SoftAdapt
      beta: 1.1         # controls strength of SoftAdapt loss coefficient updates
      interval: epoch   # update on "epoch" or "batch" basis
      frequency: 5      # number of intervals (epoch or batches) between SoftAdapt loss coefficient updates

    # or manually schedule changing of loss coefficients at the start of each training epoch
    #- _target_: nequip.train.callbacks.LossCoefficientScheduler
    #  schedule:
    #    - epoch: 2
    #      coeffs: [3, 1]
    #    - epoch: 5
    #      coeffs: [10, 1]

    # to log the loss coefficients
    - _target_: nequip.train.callbacks.LossCoefficientMonitor
      interval: epoch
      frequency: 5

# training_module refers to a NequIPLightningModule
training_module:
  _target_: nequip.train.NequIPLightningModule

  # use a MetricsManager (see docs) to construct the loss function
  loss:
    _target_: nequip.train.MetricsManager
    metrics:
      - name: peratomE_MSE
        field:
          _target_: nequip.data.PerAtomModifier
          field: total_energy
        coeff: 1
        metric:
          _target_: nequip.train.MeanSquaredError
      - name: force_MSE
        field: forces
        coeff: 1
        metric:
          _target_: nequip.train.MeanSquaredError
      # one could also use a per-type averaged metric
      # in this case, there are two types of atoms, C and H, so
      # separate per-type C and H force MSEs are computed before averaging the two
      # unlike the above that just averages all forces without accounting for the atom types
      #- name: force_MSE
      #  field: forces
      #  per_type: true
      #  coeff: 1
      #  metric:
      #    _target_: nequip.train.MeanSquaredError

  # use a MetricsManager (see docs) to construct the metrics used for monitoring
  # and influencing training, e.g. with LR schedulers or early stopping, etc
  val_metrics:
    _target_: nequip.train.MetricsManager
    metrics:
      - name: E_MSE_val
        field: total_energy
        coeff: 1
        metric:
          _target_: nequip.train.MeanSquaredError
      - name: force_MSE_val
        field: forces
        coeff: 1
        metric:
          _target_: nequip.train.MeanSquaredError
      # as before, possible to use a per-type averaged MAE
      #- name: force_MAE
      #  field: forces
      #  per_type: true
      #  coeff: 1
      #  metric:
      #    _target_: nequip.train.MeanAbsoluteError

  # we could have train_metrics and test_metrics be different from val_metrics, but it makes sense to have them be the same
  train_metrics: ${training_module.val_metrics}  # use variable interpolation
  test_metrics: ${training_module.val_metrics}  # use variable interpolation

  # any torch compatible optimizer: https://pytorch.org/docs/stable/optim.html#algorithms
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.05
    amsgrad: true

  # see options for lr_scheduler_config
  # https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule.configure_optimizers
  lr_scheduler:
    # any torch compatible lr sceduler
    scheduler:
      _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
      factor: 0.6
      patience: 10
      threshold: 0.2
      min_lr: 1e-6
    monitor: val0_epoch/weighted_sum
    interval: epoch
    frequency: 1

  # model details
  model:
    _target_: allegro.model.AllegroModel

    # === basic model params ===
    seed: 456
    model_dtype: float32
    type_names: ${model_type_names}
    r_max: 6.0

    # == bessel encoding ==
    num_bessels: 8                # number of basis functions used in the radial Bessel basis, the default of 8 usually works well
    bessel_trainable: false       # set true to train the bessel weights (default false)
    polynomial_cutoff_p: 6        # p-exponent used in polynomial cutoff function, smaller p corresponds to stronger decay with distance

    # optionally, apply additional MLP to bessel encoding
    #radial_basis_mlp_kwargs:
    #  mlp_latent_dimensions: [32]
    #  mlp_nonlinearity: silu
    #  mlp_initialization: uniform

    # === symmetry ===
    # maximum order l to use in spherical harmonics embedding, 1 is baseline (fast), 2 is more accurate, but slower, 3 highly accurate but slow
    l_max: 3
    # whether to include parity symmetry equivariance
    # allowed: o3_full, o3_restricted, so3
    parity_setting: o3_full   

    # === allegro layers ===
    # number of tensor product layers, 1-3 usually best, more is more accurate but slower
    num_layers: 3
    
    # number of tensor features, more is more accurate but slower, 1, 4, 8, 16, 64, 128 are good options to try depending on data set
    num_tensor_features: 4

    two_body_latent_kwargs:
      mlp_latent_dimensions: [16, 32, 32, 32]
      mlp_nonlinearity: silu
      mlp_initialization: uniform

    latent_kwargs:
      mlp_latent_dimensions: [32, 32]
      mlp_nonlinearity: silu
      mlp_initialization: uniform
 
    env_embed_kwargs:
      mlp_latent_dimensions: []
      mlp_nonlinearity: null
      mlp_initialization: uniform

    # === edge MLP ===
    edge_eng_kwargs:
      mlp_latent_dimensions: [32]
      mlp_nonlinearity: null
      mlp_initialization: uniform

    # average number of neighbors for edge sum normalization
    avg_num_neighbors: ${training_data_stats:num_neighbors_mean}
    
    # == per-type per-atom scales and shifts ==
    per_type_energy_shifts: ${training_data_stats:per_atom_energy_mean}
    per_type_energy_scales: ${training_data_stats:per_type_forces_rms}
    per_type_energy_scales_trainable: false
    per_type_energy_shifts_trainable: false

    # == ZBL pair potential ==
    pair_potential:
      _target_: nequip.nn.pair_potential.ZBL
      units: metal     # Ang and kcal/mol, LAMMPS unit names;  allowed values "metal" and "real"
      chemical_species: ${chemical_symbols}   # must tell ZBL the chemical species of the various model atom types

# global options
global_options:
  seed: 789
  allow_tf32: true